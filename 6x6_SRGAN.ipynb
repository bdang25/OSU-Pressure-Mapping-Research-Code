{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 510 images in the 'images_32x32' folder after removing rows with all zero values.\n"
     ]
    }
   ],
   "source": [
    "'Hi iam trying to push into github repo'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"soren.csv\"  # Ensure this path is correct\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data.drop(['Frame', 'Timestamp', 'Average Pressure (mmHg)', 'Minimum Pressure (mmHg)', \n",
    "           'Maximum Pressure (mmHg)', 'Standard Pressure Deviation (mmHg)', \n",
    "           'Median Pressure (mmHg)', 'Contact Area (m²)', 'Total Area (m²)', \n",
    "           'Estimated Force (N)', 'Range Min (mmHg)', 'Range Max (mmHg)'], axis=1, inplace=True)\n",
    "\n",
    "# Remove rows with all zero values\n",
    "data = data[(data != 0).any(axis=1)]\n",
    "\n",
    "# Create a directory to save the images\n",
    "output_folder = \"images_32x32\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Get the number of rows after filtering\n",
    "num_rows = data.shape[0]\n",
    "\n",
    "# Loop through each row in the dataset and save it as a 32x32 image\n",
    "for i in range(num_rows):\n",
    "    # Reshape the data into a 32x32 grid\n",
    "    grid_data = data.iloc[i].values.reshape(32, 32)\n",
    "    \n",
    "    # Create a plot without axis\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(grid_data, cmap='viridis', interpolation='none')\n",
    "    plt.axis('off')  # Remove axis\n",
    "    \n",
    "    # Save the image in the output folder\n",
    "    image_filename = os.path.join(output_folder, f\"pressure_image_{i+1}.png\")\n",
    "    plt.savefig(image_filename, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved {num_rows} images in the '{output_folder}' folder after removing rows with all zero values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 510 6x6 images in the 'images_6x6' folder after processing the original data.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"soren.csv\"  # Ensure this path is correct\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Frame', 'Timestamp', 'Average Pressure (mmHg)', 'Minimum Pressure (mmHg)', \n",
    "                   'Maximum Pressure (mmHg)', 'Standard Pressure Deviation (mmHg)', \n",
    "                   'Median Pressure (mmHg)', 'Contact Area (m²)', 'Total Area (m²)', \n",
    "                   'Estimated Force (N)', 'Range Min (mmHg)', 'Range Max (mmHg)']\n",
    "\n",
    "# Drop metadata columns if they exist\n",
    "data.drop(columns=[col for col in columns_to_drop if col in data.columns], axis=1, inplace=True)\n",
    "\n",
    "# Remove rows where all values are zero\n",
    "data = data[(data != 0).any(axis=1)]\n",
    "\n",
    "# Create a directory to save the 6x6 images\n",
    "output_folder_6x6 = \"images_6x6\"\n",
    "os.makedirs(output_folder_6x6, exist_ok=True)\n",
    "\n",
    "# Get the number of rows after filtering\n",
    "num_rows = data.shape[0]\n",
    "\n",
    "# Generate 6 evenly spaced indices from a 32×32 grid\n",
    "indices = np.linspace(2, 30, 6, dtype=int)  # Choosing 6 evenly spaced points in a 32x32 grid\n",
    "\n",
    "# Generate (x, y) coordinate pairs for the 6×6 grid\n",
    "selected_indices = [(indices[j], indices[k]) for j in range(6) for k in range(6)]\n",
    "\n",
    "# Loop through each row in the dataset and save it as a 6x6 image\n",
    "for i in range(num_rows):\n",
    "    # Reshape the data into a 32x32 grid\n",
    "    grid_data = data.iloc[i].values.reshape(32, 32)\n",
    "    \n",
    "    # Extract the 6x6 grid correctly\n",
    "    reduced_grid_data = np.array([\n",
    "        [grid_data[x, y] for y in np.linspace(2, 30, 6, dtype=int)]  # Select columns\n",
    "        for x in np.linspace(2, 30, 6, dtype=int)  # Select rows\n",
    "    ])\n",
    "    \n",
    "    # Create a plot without axis\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(reduced_grid_data, cmap='viridis', interpolation='none')\n",
    "    plt.axis('off')  # Remove axis\n",
    "    \n",
    "    # Save the image in the output folder\n",
    "    image_filename_6x6 = os.path.join(output_folder_6x6, f\"pressure_image_6x6_{i+1}.png\")\n",
    "    plt.savefig(image_filename_6x6, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved {num_rows} 6x6 images in the '{output_folder_6x6}' folder after processing the original data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 6×6 Grid:\n",
      "[[ 0  0  0 11 26 40]\n",
      " [ 0  0 32 42 22 10]\n",
      " [ 0  0  6  3  0  0]\n",
      " [ 0  0  5 11  5  3]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "\n",
      "Original 32×32 Grid (Subset of Selected Indices):\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(11), np.int64(26), np.int64(40)]\n",
      "[np.int64(0), np.int64(0), np.int64(32), np.int64(42), np.int64(22), np.int64(10)]\n",
      "[np.int64(0), np.int64(0), np.int64(6), np.int64(3), np.int64(0), np.int64(0)]\n",
      "[np.int64(0), np.int64(0), np.int64(5), np.int64(11), np.int64(5), np.int64(3)]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(2), np.int64(0), np.int64(0)]\n",
      "[np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0), np.int64(0)]\n"
     ]
    }
   ],
   "source": [
    "# Load the 290th 32×32 grid\n",
    "grid_data = data.iloc[500].values.reshape(32, 32)  # Adjust index (290th image)\n",
    "\n",
    "# Extract the 6×6 grid using the same method\n",
    "row_indices = np.linspace(3, 28, 6, dtype=int)  # Ensure evenly spaced\n",
    "col_indices = np.linspace(3, 28, 6, dtype=int)\n",
    "extracted_6x6 = np.array([[grid_data[x, y] for y in col_indices] for x in row_indices])\n",
    "\n",
    "# Print extracted 6x6 values\n",
    "print(\"Extracted 6×6 Grid:\")\n",
    "print(extracted_6x6)\n",
    "\n",
    "# Print corresponding values in the original 32×32 grid\n",
    "print(\"\\nOriginal 32×32 Grid (Subset of Selected Indices):\")\n",
    "for x in row_indices:\n",
    "    print([grid_data[x, y] for y in col_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m extracted_grids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Loop through each image (row) in the dataset\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m)):  \n\u001b[1;32m     13\u001b[0m     grid_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[i]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# Reshape to 32×32 grid\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     extracted_6x6 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[grid_data[x, y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m col_indices] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m row_indices])  \u001b[38;5;66;03m# Extract 6x6 grid\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define row and column indices for 6×6 selection\n",
    "row_indices = np.linspace(3, 28, 6, dtype=int)  # 6 evenly spaced rows\n",
    "col_indices = np.linspace(3, 28, 6, dtype=int)  # 6 evenly spaced columns\n",
    "\n",
    "# List to store extracted grids for all images\n",
    "extracted_grids = []\n",
    "\n",
    "# Loop through each image (row) in the dataset\n",
    "for i in range(len(data)):  \n",
    "    grid_data = data.iloc[i].values.reshape(32, 32)  # Reshape to 32×32 grid\n",
    "    extracted_6x6 = np.array([[grid_data[x, y] for y in col_indices] for x in row_indices])  # Extract 6x6 grid\n",
    "    extracted_grids.append(extracted_6x6.flatten())  # Flatten 6x6 grid into a row\n",
    "\n",
    "# Convert list to DataFrame\n",
    "extracted_df = pd.DataFrame(extracted_grids)\n",
    "\n",
    "# Save to CSV file\n",
    "extracted_df.to_csv(\"extracted_6x6_grids.csv\", index=False)\n",
    "\n",
    "print(\"CSV file 'extracted_6x6_grids.csv' has been saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Extracted 6×6 Grid:\n",
      "[[ 0  0  0 11 26 40]\n",
      " [ 0  0 32 42 22 10]\n",
      " [ 0  0  6  3  0  0]\n",
      " [ 0  0  5 11  5  3]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "\n",
      "Extracted 6×6 Grid from CSV:\n",
      "[[ 0  0  0 11 26 40]\n",
      " [ 0  0 32 42 22 10]\n",
      " [ 0  0  6  3  0  0]\n",
      " [ 0  0  5 11  5  3]\n",
      " [ 0  0  0  2  0  0]\n",
      " [ 0  0  0  0  0  0]]\n",
      "\n",
      "✅ The extracted values match! The CSV is correct.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the extracted CSV\n",
    "extracted_df = pd.read_csv(\"extracted_6x6_grids.csv\")\n",
    "\n",
    "# Choose a random image index to check (e.g., 290th image)\n",
    "image_index = 500  # Adjust to match an index in your dataset\n",
    "\n",
    "# Get the original 32x32 grid\n",
    "grid_data = data.iloc[image_index].values.reshape(32, 32)\n",
    "\n",
    "# Define row and column indices used for extraction\n",
    "row_indices = np.linspace(3, 28, 6, dtype=int)\n",
    "col_indices = np.linspace(3, 28, 6, dtype=int)\n",
    "\n",
    "# Extract 6x6 grid manually again\n",
    "manual_extracted_6x6 = np.array([[grid_data[x, y] for y in col_indices] for x in row_indices])\n",
    "\n",
    "# Load the extracted row from CSV and reshape to 6×6 for comparison\n",
    "extracted_6x6_from_csv = extracted_df.iloc[image_index].values.reshape(6, 6)\n",
    "\n",
    "# Print the original extracted 6x6 grid\n",
    "print(\"Original Extracted 6×6 Grid:\")\n",
    "print(manual_extracted_6x6)\n",
    "\n",
    "# Print the saved extracted 6x6 grid from the CSV\n",
    "print(\"\\nExtracted 6×6 Grid from CSV:\")\n",
    "print(extracted_6x6_from_csv)\n",
    "\n",
    "# Check if they are the same\n",
    "if np.array_equal(manual_extracted_6x6, extracted_6x6_from_csv):\n",
    "    print(\"\\n✅ The extracted values match! The CSV is correct.\")\n",
    "else:\n",
    "    print(\"\\n❌ Mismatch detected! Check extraction logic.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✅ Generator & Discriminator initialized.\n",
      "Low-res images shape: torch.Size([510, 3, 6, 6])\n",
      "High-res images shape: torch.Size([510, 3, 32, 32])\n",
      "Epoch [1/250], D Loss: 0.7568, G Loss: 1.9224\n",
      "Epoch [2/250], D Loss: 0.2392, G Loss: 3.0120\n",
      "Epoch [3/250], D Loss: 0.3755, G Loss: 2.8876\n",
      "Epoch [4/250], D Loss: 4.2517, G Loss: 0.9073\n",
      "Epoch [5/250], D Loss: 0.2801, G Loss: 2.4069\n",
      "Epoch [6/250], D Loss: 1.2545, G Loss: 1.0309\n",
      "Epoch [7/250], D Loss: 0.9426, G Loss: 1.1877\n",
      "Epoch [8/250], D Loss: 1.0315, G Loss: 1.2157\n",
      "Epoch [9/250], D Loss: 1.1781, G Loss: 1.0051\n",
      "Epoch [10/250], D Loss: 1.2780, G Loss: 0.8916\n",
      "Epoch [11/250], D Loss: 1.3572, G Loss: 0.8356\n",
      "Epoch [12/250], D Loss: 1.3438, G Loss: 0.8435\n",
      "Epoch [13/250], D Loss: 1.3243, G Loss: 0.7921\n",
      "Epoch [14/250], D Loss: 1.3045, G Loss: 0.7798\n",
      "Epoch [15/250], D Loss: 1.2894, G Loss: 0.7628\n",
      "Epoch [16/250], D Loss: 1.2787, G Loss: 0.8261\n",
      "Epoch [17/250], D Loss: 1.2751, G Loss: 0.9118\n",
      "Epoch [18/250], D Loss: 1.2640, G Loss: 0.9101\n",
      "Epoch [19/250], D Loss: 1.2644, G Loss: 0.9026\n",
      "Epoch [20/250], D Loss: 1.2399, G Loss: 0.8964\n",
      "Epoch [21/250], D Loss: 1.3818, G Loss: 0.7831\n",
      "Epoch [22/250], D Loss: 1.2585, G Loss: 0.9950\n",
      "Epoch [23/250], D Loss: 1.2115, G Loss: 0.9586\n",
      "Epoch [24/250], D Loss: 1.2536, G Loss: 0.9519\n",
      "Epoch [25/250], D Loss: 1.3383, G Loss: 0.8446\n",
      "Epoch [26/250], D Loss: 1.3156, G Loss: 0.8536\n",
      "Epoch [27/250], D Loss: 1.1817, G Loss: 0.9651\n",
      "Epoch [28/250], D Loss: 0.9192, G Loss: 1.0545\n",
      "Epoch [29/250], D Loss: 1.2797, G Loss: 0.9711\n",
      "Epoch [30/250], D Loss: 1.1967, G Loss: 1.0613\n",
      "Epoch [31/250], D Loss: 1.2981, G Loss: 0.9544\n",
      "Epoch [32/250], D Loss: 1.2737, G Loss: 1.0131\n",
      "Epoch [33/250], D Loss: 1.4767, G Loss: 0.8999\n",
      "Epoch [34/250], D Loss: 1.0831, G Loss: 1.2106\n",
      "Epoch [35/250], D Loss: 1.0713, G Loss: 1.2120\n",
      "Epoch [36/250], D Loss: 1.0875, G Loss: 1.1969\n",
      "Epoch [37/250], D Loss: 1.1755, G Loss: 1.0933\n",
      "Epoch [38/250], D Loss: 1.3108, G Loss: 1.0282\n",
      "Epoch [39/250], D Loss: 1.1361, G Loss: 1.1599\n",
      "Epoch [40/250], D Loss: 1.4047, G Loss: 0.8562\n",
      "Epoch [41/250], D Loss: 1.4445, G Loss: 0.8512\n",
      "Epoch [42/250], D Loss: 0.9764, G Loss: 1.2081\n",
      "Epoch [43/250], D Loss: 0.9247, G Loss: 1.3712\n",
      "Epoch [44/250], D Loss: 1.3134, G Loss: 0.8349\n",
      "Epoch [45/250], D Loss: 1.0762, G Loss: 1.2391\n",
      "Epoch [46/250], D Loss: 1.2095, G Loss: 0.9516\n",
      "Epoch [47/250], D Loss: 1.3813, G Loss: 0.8730\n",
      "Epoch [48/250], D Loss: 1.0731, G Loss: 1.3267\n",
      "Epoch [49/250], D Loss: 1.4517, G Loss: 0.7824\n",
      "Epoch [50/250], D Loss: 1.7059, G Loss: 0.6457\n",
      "✅ Saved checkpoint at epoch 50\n",
      "Epoch [51/250], D Loss: 1.4909, G Loss: 0.7348\n",
      "Epoch [52/250], D Loss: 1.3892, G Loss: 0.8456\n",
      "Epoch [53/250], D Loss: 1.5683, G Loss: 0.7407\n",
      "Epoch [54/250], D Loss: 1.4830, G Loss: 0.7499\n",
      "Epoch [55/250], D Loss: 1.6179, G Loss: 0.6254\n",
      "Epoch [56/250], D Loss: 1.5520, G Loss: 0.7065\n",
      "Epoch [57/250], D Loss: 1.4977, G Loss: 0.6717\n",
      "Epoch [58/250], D Loss: 1.4460, G Loss: 0.6706\n",
      "Epoch [59/250], D Loss: 1.4389, G Loss: 0.6733\n",
      "Epoch [60/250], D Loss: 1.4128, G Loss: 0.6842\n",
      "Epoch [61/250], D Loss: 1.4409, G Loss: 0.6511\n",
      "Epoch [62/250], D Loss: 1.3928, G Loss: 0.6550\n",
      "Epoch [63/250], D Loss: 1.3510, G Loss: 0.6389\n",
      "Epoch [64/250], D Loss: 1.4254, G Loss: 0.6290\n",
      "Epoch [65/250], D Loss: 1.2748, G Loss: 0.6822\n",
      "Epoch [66/250], D Loss: 1.4099, G Loss: 0.6199\n",
      "Epoch [67/250], D Loss: 1.3535, G Loss: 0.6295\n",
      "Epoch [68/250], D Loss: 1.2831, G Loss: 0.6294\n",
      "Epoch [69/250], D Loss: 1.2731, G Loss: 0.6365\n",
      "Epoch [70/250], D Loss: 1.2589, G Loss: 0.6530\n",
      "Epoch [71/250], D Loss: 1.2529, G Loss: 0.6539\n",
      "Epoch [72/250], D Loss: 1.2896, G Loss: 0.6543\n",
      "Epoch [73/250], D Loss: 1.2789, G Loss: 0.6502\n",
      "Epoch [74/250], D Loss: 1.2825, G Loss: 0.6567\n",
      "Epoch [75/250], D Loss: 1.2985, G Loss: 0.6231\n",
      "Epoch [76/250], D Loss: 1.2617, G Loss: 0.6605\n",
      "Epoch [77/250], D Loss: 1.2999, G Loss: 0.6387\n",
      "Epoch [78/250], D Loss: 1.3044, G Loss: 0.6205\n",
      "Epoch [79/250], D Loss: 1.3722, G Loss: 0.5909\n",
      "Epoch [80/250], D Loss: 1.2851, G Loss: 0.6454\n",
      "Epoch [81/250], D Loss: 1.2912, G Loss: 0.6413\n",
      "Epoch [82/250], D Loss: 1.2906, G Loss: 0.6588\n",
      "Epoch [83/250], D Loss: 1.3115, G Loss: 0.6312\n",
      "Epoch [84/250], D Loss: 1.2845, G Loss: 0.6526\n",
      "Epoch [85/250], D Loss: 1.2460, G Loss: 0.6797\n",
      "Epoch [86/250], D Loss: 1.3351, G Loss: 0.6074\n",
      "Epoch [87/250], D Loss: 1.4156, G Loss: 0.6159\n",
      "Epoch [88/250], D Loss: 1.3047, G Loss: 0.6410\n",
      "Epoch [89/250], D Loss: 1.0993, G Loss: 0.7337\n",
      "Epoch [90/250], D Loss: 1.4461, G Loss: 0.5907\n",
      "Epoch [91/250], D Loss: 1.3808, G Loss: 0.6429\n",
      "Epoch [92/250], D Loss: 1.2465, G Loss: 0.6730\n",
      "Epoch [93/250], D Loss: 1.3642, G Loss: 0.6718\n",
      "Epoch [94/250], D Loss: 1.3296, G Loss: 0.7043\n",
      "Epoch [95/250], D Loss: 1.3342, G Loss: 0.7057\n",
      "Epoch [96/250], D Loss: 1.2634, G Loss: 0.7440\n",
      "Epoch [97/250], D Loss: 1.3535, G Loss: 0.7238\n",
      "Epoch [98/250], D Loss: 1.3831, G Loss: 0.7224\n",
      "Epoch [99/250], D Loss: 1.0407, G Loss: 0.8754\n",
      "Epoch [100/250], D Loss: 1.3714, G Loss: 0.7295\n",
      "✅ Saved checkpoint at epoch 100\n",
      "Epoch [101/250], D Loss: 1.3634, G Loss: 0.7640\n",
      "Epoch [102/250], D Loss: 1.3733, G Loss: 0.7643\n",
      "Epoch [103/250], D Loss: 1.3675, G Loss: 0.7360\n",
      "Epoch [104/250], D Loss: 1.3578, G Loss: 0.7944\n",
      "Epoch [105/250], D Loss: 1.3712, G Loss: 0.7028\n",
      "Epoch [106/250], D Loss: 1.3741, G Loss: 0.7298\n",
      "Epoch [107/250], D Loss: 1.3647, G Loss: 0.7136\n",
      "Epoch [108/250], D Loss: 1.3449, G Loss: 0.7823\n",
      "Epoch [109/250], D Loss: 1.3781, G Loss: 0.7053\n",
      "Epoch [110/250], D Loss: 1.3605, G Loss: 0.7137\n",
      "Epoch [111/250], D Loss: 1.4123, G Loss: 0.6787\n",
      "Epoch [112/250], D Loss: 1.3449, G Loss: 0.7119\n",
      "Epoch [113/250], D Loss: 1.3743, G Loss: 0.7016\n",
      "Epoch [114/250], D Loss: 1.3442, G Loss: 0.7049\n",
      "Epoch [115/250], D Loss: 1.2865, G Loss: 0.7358\n",
      "Epoch [116/250], D Loss: 1.3358, G Loss: 0.6504\n",
      "Epoch [117/250], D Loss: 1.3699, G Loss: 0.6338\n",
      "Epoch [118/250], D Loss: 1.3357, G Loss: 0.6270\n",
      "Epoch [119/250], D Loss: 1.3024, G Loss: 0.6548\n",
      "Epoch [120/250], D Loss: 1.3585, G Loss: 0.6370\n",
      "Epoch [121/250], D Loss: 1.2834, G Loss: 0.6549\n",
      "Epoch [122/250], D Loss: 1.2792, G Loss: 0.6476\n",
      "Epoch [123/250], D Loss: 1.3423, G Loss: 0.6339\n",
      "Epoch [124/250], D Loss: 1.2998, G Loss: 0.6112\n",
      "Epoch [125/250], D Loss: 1.3036, G Loss: 0.6244\n",
      "Epoch [126/250], D Loss: 1.3063, G Loss: 0.6224\n",
      "Epoch [127/250], D Loss: 1.3522, G Loss: 0.6037\n",
      "Epoch [128/250], D Loss: 1.3367, G Loss: 0.6254\n",
      "Epoch [129/250], D Loss: 1.2120, G Loss: 0.7173\n",
      "Epoch [130/250], D Loss: 1.3324, G Loss: 0.6034\n",
      "Epoch [131/250], D Loss: 1.3032, G Loss: 0.6477\n",
      "Epoch [132/250], D Loss: 1.3475, G Loss: 0.6288\n",
      "Epoch [133/250], D Loss: 1.3138, G Loss: 0.6410\n",
      "Epoch [134/250], D Loss: 1.2774, G Loss: 0.6538\n",
      "Epoch [135/250], D Loss: 1.3283, G Loss: 0.6300\n",
      "Epoch [136/250], D Loss: 1.3450, G Loss: 0.6288\n",
      "Epoch [137/250], D Loss: 1.3287, G Loss: 0.6403\n",
      "Epoch [138/250], D Loss: 1.3104, G Loss: 0.6378\n",
      "Epoch [139/250], D Loss: 1.2881, G Loss: 0.6397\n",
      "Epoch [140/250], D Loss: 1.3148, G Loss: 0.6340\n",
      "Epoch [141/250], D Loss: 1.2742, G Loss: 0.6316\n",
      "Epoch [142/250], D Loss: 1.3448, G Loss: 0.6196\n",
      "Epoch [143/250], D Loss: 1.2720, G Loss: 0.6598\n",
      "Epoch [144/250], D Loss: 1.2935, G Loss: 0.6472\n",
      "Epoch [145/250], D Loss: 1.2785, G Loss: 0.6614\n",
      "Epoch [146/250], D Loss: 1.2959, G Loss: 0.6471\n",
      "Epoch [147/250], D Loss: 1.2703, G Loss: 0.6540\n",
      "Epoch [148/250], D Loss: 1.2993, G Loss: 0.6508\n",
      "Epoch [149/250], D Loss: 1.2709, G Loss: 0.6618\n",
      "Epoch [150/250], D Loss: 1.2994, G Loss: 0.6664\n",
      "✅ Saved checkpoint at epoch 150\n",
      "Epoch [151/250], D Loss: 1.3247, G Loss: 0.6520\n",
      "Epoch [152/250], D Loss: 1.2895, G Loss: 0.6528\n",
      "Epoch [153/250], D Loss: 1.2856, G Loss: 0.6511\n",
      "Epoch [154/250], D Loss: 1.2962, G Loss: 0.6715\n",
      "Epoch [155/250], D Loss: 1.3147, G Loss: 0.6566\n",
      "Epoch [156/250], D Loss: 1.2917, G Loss: 0.6696\n",
      "Epoch [157/250], D Loss: 1.2973, G Loss: 0.6634\n",
      "Epoch [158/250], D Loss: 1.2920, G Loss: 0.6663\n",
      "Epoch [159/250], D Loss: 1.3092, G Loss: 0.6685\n",
      "Epoch [160/250], D Loss: 1.2855, G Loss: 0.6665\n",
      "Epoch [161/250], D Loss: 1.2859, G Loss: 0.6680\n",
      "Epoch [162/250], D Loss: 1.2889, G Loss: 0.6588\n",
      "Epoch [163/250], D Loss: 1.2947, G Loss: 0.6635\n",
      "Epoch [164/250], D Loss: 1.2762, G Loss: 0.7108\n",
      "Epoch [165/250], D Loss: 1.3251, G Loss: 0.6464\n",
      "Epoch [166/250], D Loss: 1.3060, G Loss: 0.6723\n",
      "Epoch [167/250], D Loss: 1.3035, G Loss: 0.6666\n",
      "Epoch [168/250], D Loss: 1.2759, G Loss: 0.6895\n",
      "Epoch [169/250], D Loss: 1.2873, G Loss: 0.6792\n",
      "Epoch [170/250], D Loss: 1.3067, G Loss: 0.6641\n",
      "Epoch [171/250], D Loss: 1.3027, G Loss: 0.6666\n",
      "Epoch [172/250], D Loss: 1.2888, G Loss: 0.6668\n",
      "Epoch [173/250], D Loss: 1.2949, G Loss: 0.6708\n",
      "Epoch [174/250], D Loss: 1.3050, G Loss: 0.6717\n",
      "Epoch [175/250], D Loss: 1.3524, G Loss: 0.6587\n",
      "Epoch [176/250], D Loss: 1.3062, G Loss: 0.6729\n",
      "Epoch [177/250], D Loss: 1.3127, G Loss: 0.6702\n",
      "Epoch [178/250], D Loss: 1.2381, G Loss: 0.7395\n",
      "Epoch [179/250], D Loss: 1.3775, G Loss: 0.6310\n",
      "Epoch [180/250], D Loss: 1.3484, G Loss: 0.6558\n",
      "Epoch [181/250], D Loss: 1.2978, G Loss: 0.6837\n",
      "Epoch [182/250], D Loss: 1.3149, G Loss: 0.6782\n",
      "Epoch [183/250], D Loss: 1.2745, G Loss: 0.6919\n",
      "Epoch [184/250], D Loss: 1.2534, G Loss: 0.7057\n",
      "Epoch [185/250], D Loss: 1.2991, G Loss: 0.7027\n",
      "Epoch [186/250], D Loss: 1.2836, G Loss: 0.6943\n",
      "Epoch [187/250], D Loss: 1.2631, G Loss: 0.6827\n",
      "Epoch [188/250], D Loss: 1.2635, G Loss: 0.6978\n",
      "Epoch [189/250], D Loss: 1.2554, G Loss: 0.7060\n",
      "Epoch [190/250], D Loss: 1.3029, G Loss: 0.6682\n",
      "Epoch [191/250], D Loss: 1.2675, G Loss: 0.7006\n",
      "Epoch [192/250], D Loss: 1.3001, G Loss: 0.6707\n",
      "Epoch [193/250], D Loss: 1.2399, G Loss: 0.7093\n",
      "Epoch [194/250], D Loss: 1.2599, G Loss: 0.6849\n",
      "Epoch [195/250], D Loss: 1.3193, G Loss: 0.6648\n",
      "Epoch [196/250], D Loss: 1.2149, G Loss: 0.7364\n",
      "Epoch [197/250], D Loss: 1.2483, G Loss: 0.7075\n",
      "Epoch [198/250], D Loss: 1.2478, G Loss: 0.7117\n",
      "Epoch [199/250], D Loss: 1.2587, G Loss: 0.7100\n",
      "Epoch [200/250], D Loss: 1.2813, G Loss: 0.6804\n",
      "✅ Saved checkpoint at epoch 200\n",
      "Epoch [201/250], D Loss: 1.2722, G Loss: 0.7381\n",
      "Epoch [202/250], D Loss: 1.3190, G Loss: 0.6807\n",
      "Epoch [203/250], D Loss: 1.3129, G Loss: 0.6802\n",
      "Epoch [204/250], D Loss: 1.2176, G Loss: 0.7227\n",
      "Epoch [205/250], D Loss: 1.2776, G Loss: 0.6716\n",
      "Epoch [206/250], D Loss: 1.3087, G Loss: 0.6696\n",
      "Epoch [207/250], D Loss: 1.2546, G Loss: 0.7154\n",
      "Epoch [208/250], D Loss: 1.2977, G Loss: 0.6652\n",
      "Epoch [209/250], D Loss: 1.2262, G Loss: 0.7145\n",
      "Epoch [210/250], D Loss: 1.2064, G Loss: 0.7289\n",
      "Epoch [211/250], D Loss: 1.2291, G Loss: 0.7192\n",
      "Epoch [212/250], D Loss: 1.3202, G Loss: 0.6598\n",
      "Epoch [213/250], D Loss: 1.2839, G Loss: 0.6834\n",
      "Epoch [214/250], D Loss: 1.2121, G Loss: 0.7414\n",
      "Epoch [215/250], D Loss: 1.2139, G Loss: 0.7026\n",
      "Epoch [216/250], D Loss: 1.2606, G Loss: 0.6700\n",
      "Epoch [217/250], D Loss: 1.2002, G Loss: 0.7824\n",
      "Epoch [218/250], D Loss: 1.2677, G Loss: 0.6808\n",
      "Epoch [219/250], D Loss: 1.2599, G Loss: 0.6956\n",
      "Epoch [220/250], D Loss: 1.2854, G Loss: 0.6858\n",
      "Epoch [221/250], D Loss: 1.2817, G Loss: 0.6839\n",
      "Epoch [222/250], D Loss: 1.2861, G Loss: 0.7051\n",
      "Epoch [223/250], D Loss: 1.2040, G Loss: 0.7317\n",
      "Epoch [224/250], D Loss: 1.2155, G Loss: 0.7077\n",
      "Epoch [225/250], D Loss: 1.2509, G Loss: 0.7312\n",
      "Epoch [226/250], D Loss: 1.2008, G Loss: 0.7180\n",
      "Epoch [227/250], D Loss: 1.1837, G Loss: 0.7066\n",
      "Epoch [228/250], D Loss: 1.1598, G Loss: 0.6899\n",
      "Epoch [229/250], D Loss: 1.2254, G Loss: 0.6995\n",
      "Epoch [230/250], D Loss: 1.2139, G Loss: 0.7127\n",
      "Epoch [231/250], D Loss: 1.2028, G Loss: 0.7217\n",
      "Epoch [232/250], D Loss: 1.2057, G Loss: 0.7207\n",
      "Epoch [233/250], D Loss: 1.2358, G Loss: 0.7233\n",
      "Epoch [234/250], D Loss: 1.2419, G Loss: 0.7065\n",
      "Epoch [235/250], D Loss: 1.1918, G Loss: 0.7499\n",
      "Epoch [236/250], D Loss: 1.2503, G Loss: 0.7073\n",
      "Epoch [237/250], D Loss: 1.1860, G Loss: 0.7512\n",
      "Epoch [238/250], D Loss: 1.1869, G Loss: 0.7502\n",
      "Epoch [239/250], D Loss: 1.1996, G Loss: 0.7598\n",
      "Epoch [240/250], D Loss: 1.1783, G Loss: 0.7608\n",
      "Epoch [241/250], D Loss: 1.1835, G Loss: 0.7561\n",
      "Epoch [242/250], D Loss: 1.2511, G Loss: 0.7107\n",
      "Epoch [243/250], D Loss: 1.1769, G Loss: 0.7759\n",
      "Epoch [244/250], D Loss: 1.1748, G Loss: 0.7861\n",
      "Epoch [245/250], D Loss: 1.2179, G Loss: 0.7548\n",
      "Epoch [246/250], D Loss: 1.1745, G Loss: 0.7446\n",
      "Epoch [247/250], D Loss: 1.1966, G Loss: 0.7377\n",
      "Epoch [248/250], D Loss: 1.2214, G Loss: 0.7354\n",
      "Epoch [249/250], D Loss: 1.1487, G Loss: 0.7875\n",
      "Epoch [250/250], D Loss: 1.2767, G Loss: 0.6916\n",
      "✅ Saved checkpoint at epoch 250\n",
      "✅ Training Complete! Saving Final Models...\n",
      "✅ Models Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ===========================\n",
    "# Device Setup\n",
    "# ===========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ===========================\n",
    "# Generator Model (6×6 → 32×32)\n",
    "# ===========================\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Use ConvTranspose2d-based upsampling for learned upscaling.\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: 6×6 → remains 6×6\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Upsample: 6×6 → 12×12\n",
    "            nn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Upsample: 12×12 → 24×24\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Upsample: 24×24 → 32×32 (Here we use kernel_size=4, stride=2, padding=1 would give 48×48,\n",
    "            # so we change it to a smaller scaling factor. One way is to use an interpolation here.)\n",
    "            # Option 1: Use ConvTranspose2d with modified parameters:\n",
    "            # nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            # Option 2: Use Upsample with bilinear interpolation.\n",
    "            nn.Upsample(scale_factor=4/3, mode=\"bilinear\", align_corners=True),  # 24×24 → 32×32\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ===========================\n",
    "# Discriminator Model (for 32×32 images)\n",
    "# ===========================\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ===========================\n",
    "# Initialize Models\n",
    "# ===========================\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "print(\"✅ Generator & Discriminator initialized.\")\n",
    "\n",
    "# ===========================\n",
    "# Loss Functions & Optimizers\n",
    "# ===========================\n",
    "criterion = nn.BCELoss()  # Adversarial loss\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=5e-5, betas=(0.5, 0.999))\n",
    "\n",
    "# ===========================\n",
    "# Perceptual Loss (VGG16-based)\n",
    "# ===========================\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16]\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg.to(device)\n",
    "        self.resize = nn.Upsample(size=(224, 224), mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        # Resize both generated and target to 224x224\n",
    "        gen_resized = self.resize(generated)\n",
    "        target_resized = self.resize(target)\n",
    "        return nn.functional.mse_loss(self.vgg(gen_resized), self.vgg(target_resized))\n",
    "\n",
    "perceptual_loss = PerceptualLoss()\n",
    "\n",
    "# ===========================\n",
    "# (Optional) Edge Loss to Improve Sharpness\n",
    "def edge_loss(generated, target):\n",
    "    # Create a Sobel filter for one channel\n",
    "    sobel_x = torch.tensor([[1, 0, -1],\n",
    "                             [2, 0, -2],\n",
    "                             [1, 0, -1]], dtype=torch.float32)\n",
    "    sobel_y = torch.tensor([[1, 2, 1],\n",
    "                             [0, 0, 0],\n",
    "                             [-1, -2, -1]], dtype=torch.float32)\n",
    "    \n",
    "    # Reshape to [1, 1, 3, 3] and repeat for 3 channels\n",
    "    sobel_x = sobel_x.unsqueeze(0).unsqueeze(0).to(generated.device)  # shape [1,1,3,3]\n",
    "    sobel_y = sobel_y.unsqueeze(0).unsqueeze(0).to(generated.device)\n",
    "    \n",
    "    # Repeat filters along the channel dimension for each input channel\n",
    "    sobel_x = sobel_x.repeat(3, 1, 1, 1)  # shape becomes [3,1,3,3]\n",
    "    sobel_y = sobel_y.repeat(3, 1, 1, 1)\n",
    "    \n",
    "    # Apply convolution using groups=3 to apply the filter separately for each channel\n",
    "    gen_edges_x = nn.functional.conv2d(generated, sobel_x, padding=1, groups=3)\n",
    "    gen_edges_y = nn.functional.conv2d(generated, sobel_y, padding=1, groups=3)\n",
    "    target_edges_x = nn.functional.conv2d(target, sobel_x, padding=1, groups=3)\n",
    "    target_edges_y = nn.functional.conv2d(target, sobel_y, padding=1, groups=3)\n",
    "    \n",
    "    # Use L1 loss for edge differences\n",
    "    return nn.functional.l1_loss(gen_edges_x, target_edges_x) + nn.functional.l1_loss(gen_edges_y, target_edges_y)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Data Preparation Functions\n",
    "# ===========================\n",
    "# For low-resolution images (Generator Input): Resize to 6x6\n",
    "def prepare_low_res(folder_path):\n",
    "    images = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((6, 6)),  # Force low-res images to 6x6\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.png'):\n",
    "            img = Image.open(os.path.join(folder_path, filename)).convert(\"RGB\")\n",
    "            img = transform(img)\n",
    "            images.append(img)\n",
    "    return torch.stack(images)\n",
    "\n",
    "# For high-resolution images (Target): Resize to 32x32\n",
    "def prepare_high_res(folder_path):\n",
    "    images = []\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),  # Target is 32x32\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.png'):\n",
    "            img = Image.open(os.path.join(folder_path, filename)).convert(\"RGB\")\n",
    "            img = transform(img)\n",
    "            images.append(img)\n",
    "    return torch.stack(images)\n",
    "\n",
    "# Load Data\n",
    "low_res_images = prepare_low_res('images_6x6').to(device)\n",
    "high_res_images = prepare_high_res('images_32x32').to(device)\n",
    "print(f\"Low-res images shape: {low_res_images.shape}\")\n",
    "print(f\"High-res images shape: {high_res_images.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# Training Loop\n",
    "# ===========================\n",
    "num_epochs = 250\n",
    "batch_size = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(low_res_images), batch_size):\n",
    "        low_res = low_res_images[i:i+batch_size].to(device)   # Expected shape: [B, 3, 6, 6]\n",
    "        high_res = high_res_images[i:i+batch_size].to(device)  # Expected shape: [B, 3, 32, 32]\n",
    "\n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_output = discriminator(high_res).view(-1)\n",
    "        d_loss_real = criterion(real_output, torch.ones_like(real_output).to(device))\n",
    "        \n",
    "        fake_image = generator(low_res)\n",
    "        fake_output = discriminator(fake_image.detach()).view(-1)\n",
    "        d_loss_fake = criterion(fake_output, torch.zeros_like(fake_output).to(device))\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator (2 updates per discriminator update)\n",
    "        for _ in range(2):\n",
    "            optimizer_G.zero_grad()\n",
    "            fake_image = generator(low_res)\n",
    "            fake_output = discriminator(fake_image).view(-1)\n",
    "            g_bce_loss = criterion(fake_output, torch.ones_like(fake_output).to(device))\n",
    "            g_perceptual_loss = perceptual_loss(fake_image, high_res)\n",
    "            g_edge_loss = edge_loss(fake_image, high_res)\n",
    "            g_loss = g_bce_loss + 0.3 * g_perceptual_loss + 0.1 * g_edge_loss\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Save checkpoint every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        torch.save(generator.state_dict(), f\"models/generator_epoch{epoch+1}.pth\")\n",
    "        torch.save(discriminator.state_dict(), f\"models/discriminator_epoch{epoch+1}.pth\")\n",
    "        print(f\"✅ Saved checkpoint at epoch {epoch+1}\")\n",
    "\n",
    "print(\"✅ Training Complete! Saving Final Models...\")\n",
    "torch.save(generator.state_dict(), \"models/generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"models/discriminator.pth\")\n",
    "print(\"✅ Models Saved Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransforms\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Make sure to define or import your Generator class before using it\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# from your_model_file import Generator\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load Trained Generator\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mGenerator\u001b[49m()\n\u001b[1;32m     11\u001b[0m generator\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/generator.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m generator\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Make sure to define or import your Generator class before using it\n",
    "# from your_model_file import Generator\n",
    "\n",
    "# Load Trained Generator\n",
    "generator = Generator()\n",
    "generator.load_state_dict(torch.load(\"models/generator.pth\"))\n",
    "generator.eval()\n",
    "\n",
    "# Define folders\n",
    "input_folder = \"images_6x6\"\n",
    "output_folder = \"upscaled_250_32x32\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Correct Transformations for low-res image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((6, 6)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Match training normalization\n",
    "])\n",
    "\n",
    "# Process images and save with new file names\n",
    "for i, filename in enumerate(os.listdir(input_folder), start=1):\n",
    "    if filename.endswith(\".png\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        test_image = Image.open(input_path).convert(\"RGB\")\n",
    "        low_res_input = transform(test_image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Generate upscaled 32×32 image\n",
    "        with torch.no_grad():\n",
    "            upscaled_image = generator(low_res_input).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            # Convert from [-1, 1] to [0, 1]\n",
    "            upscaled_image = (upscaled_image + 1) / 2\n",
    "\n",
    "            # Convert tensor to PIL image\n",
    "            upscaled_image = transforms.ToPILImage()(upscaled_image)\n",
    "            \n",
    "            # Create the new file name following the pattern\n",
    "            new_filename = f\"pressure_image_32x32_{i}.png\"\n",
    "            output_path = os.path.join(output_folder, new_filename)\n",
    "            \n",
    "            # Save the upscaled image\n",
    "            upscaled_image.save(output_path)\n",
    "\n",
    "        print(f\"✅ Upscaled and saved: {output_path}\")\n",
    "\n",
    "print(\"🚀 All images successfully upscaled to upscaled_250_32×32 and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'low_res_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m     visualize_images(low_res\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), generated_high_res, original_high_res, psnr_value, rmse_value)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mevaluate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m460\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace 0 with the desired index\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m, in \u001b[0;36mevaluate_image\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_image\u001b[39m(index):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Ensure index is within bounds\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlow_res_images\u001b[49m):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex out of range. Please select a valid index.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'low_res_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to visualize images\n",
    "def visualize_images(low_res, generated_high_res, original_high_res, psnr_value, rmse_value):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].imshow(low_res.permute(1, 2, 0).numpy())\n",
    "    axs[0].set_title('Low Resolution Image')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(generated_high_res.permute(1, 2, 0).detach().numpy())\n",
    "    axs[1].set_title(f'Generated High Res\\nPSNR: {psnr_value:.2f}, RMSE: {rmse_value:.2f}')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(original_high_res.permute(1, 2, 0).numpy())\n",
    "    axs[2].set_title('Original High Res Image')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate and display images\n",
    "def evaluate_image(index):\n",
    "    # Ensure index is within bounds\n",
    "    if index < 0 or index >= len(low_res_images):\n",
    "        print(\"Index out of range. Please select a valid index.\")\n",
    "        return\n",
    "\n",
    "    # Get the images\n",
    "    low_res = low_res_images[index].unsqueeze(0)  # Add batch dimension\n",
    "    original_high_res = high_res_images[index]\n",
    "\n",
    "    # Generate high-resolution image\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        generated_high_res = generator(low_res).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    # Calculate PSNR and RMSE\n",
    "    psnr_value = psnr(original_high_res.numpy(), generated_high_res.numpy())\n",
    "    rmse_value = np.sqrt(mse(original_high_res.numpy(), generated_high_res.numpy()))\n",
    "\n",
    "    # Visualize images and metrics\n",
    "    visualize_images(low_res.squeeze(0), generated_high_res, original_high_res, psnr_value, rmse_value)\n",
    "\n",
    "# Example usage\n",
    "evaluate_image(460)  # Replace 0 with the desired index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Function to save the generated high-res images\n",
    "def save_generated_images(output_folder):\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Loop through all low-res images\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    for i, low_res in enumerate(low_res_images):\n",
    "        # Add batch dimension\n",
    "        low_res = low_res.unsqueeze(0)\n",
    "\n",
    "        # Generate high-resolution image\n",
    "        with torch.no_grad():\n",
    "            generated_high_res = generator(low_res).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Convert to NumPy array and scale to 0-255\n",
    "        generated_image = (generated_high_res.permute(1, 2, 0).detach().numpy() * 255).astype(np.uint8)\n",
    "\n",
    "        # Convert to PIL image for saving\n",
    "        pil_image = Image.fromarray(generated_image)\n",
    "\n",
    "        # Save the image\n",
    "        image_path = os.path.join(output_folder, f'gen_img_{i+1}.png')\n",
    "        pil_image.save(image_path)\n",
    "\n",
    "        print(f\"Saved: {image_path}\")\n",
    "\n",
    "# Example usage\n",
    "output_folder = \"generated_6\"\n",
    "save_generated_images(output_folder)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m psnr_values, rmse_values\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Evaluate metrics for all images\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m psnr_values_all, rmse_values_all \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_all_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Function to plot PSNR and RMSE with enhanced inbuilt style\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m, in \u001b[0;36mevaluate_all_images\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m psnr_values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m rmse_values \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[43mgenerator\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set generator to evaluation mode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(low_res_images)):\n\u001b[1;32m      8\u001b[0m     low_res \u001b[38;5;241m=\u001b[39m low_res_images[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to evaluate PSNR and RMSE for all images\n",
    "def evaluate_all_images():\n",
    "    psnr_values = []\n",
    "    rmse_values = []\n",
    "\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    for i in range(len(low_res_images)):\n",
    "        low_res = low_res_images[i].unsqueeze(0)  # Add batch dimension\n",
    "        original_high_res = high_res_images[i]\n",
    "\n",
    "        # Generate high-resolution image\n",
    "        with torch.no_grad():\n",
    "            generated_high_res = generator(low_res).squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Calculate PSNR and RMSE\n",
    "        psnr_value = psnr(original_high_res.numpy(), generated_high_res.numpy())\n",
    "        rmse_value = np.sqrt(mse(original_high_res.numpy(), generated_high_res.numpy()))\n",
    "\n",
    "        # Append metrics to the lists\n",
    "        psnr_values.append(psnr_value)\n",
    "        rmse_values.append(rmse_value)\n",
    "\n",
    "    return psnr_values, rmse_values\n",
    "\n",
    "# Evaluate metrics for all images\n",
    "psnr_values_all, rmse_values_all = evaluate_all_images()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot PSNR and RMSE with enhanced inbuilt style\n",
    "def plot_metrics_with_enhanced_style(psnr_values, rmse_values):\n",
    "    # Use a professional built-in style\n",
    "    plt.style.use('fivethirtyeight')  # A clean, visually pleasing style\n",
    "\n",
    "    indices = range(len(psnr_values))  # Image indices\n",
    "\n",
    "    # Plot PSNR values\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(\n",
    "        indices, psnr_values, label='PSNR (dB)', color='#007acc',\n",
    "        marker='o', markersize=5, linewidth=2, alpha=0.9\n",
    "    )\n",
    "    plt.fill_between(indices, psnr_values, color='#007acc', alpha=0.2)  # Add shading for emphasis\n",
    "    plt.xlabel('Image Index', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('PSNR (dB)', fontsize=14, fontweight='bold')\n",
    "    plt.title('PSNR vs. Image Index', fontsize=18, fontweight='bold')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12, loc='upper right', frameon=True, shadow=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot RMSE values\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(\n",
    "        indices, rmse_values, label='RMSE', color='#ff5733',\n",
    "        marker='s', markersize=5, linewidth=2, alpha=0.9\n",
    "    )\n",
    "    plt.fill_between(indices, rmse_values, color='#ff5733', alpha=0.2)  # Add shading for emphasis\n",
    "    plt.xlabel('Image Index', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('RMSE', fontsize=14, fontweight='bold')\n",
    "    plt.title('RMSE vs. Image Index', fontsize=18, fontweight='bold')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(fontsize=12, loc='upper right', frameon=True, shadow=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_metrics_with_enhanced_style(psnr_values_all, rmse_values_all)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
